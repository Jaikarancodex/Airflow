## ğŸš€ Ultraâ€‘Light Apache Airflow (Optimized for 8 GB RAM)

This setup is **clean, fast, laptopâ€‘friendly**, and **perfect for Airflow basics â†’ intermediate practice**.
We intentionally **remove Celery, Redis, Triggerer, Flower** and use **LocalExecutor**.

---

## ğŸ¯ What this optimized setup gives you

âœ… Only **3 containers** (Postgres, Scheduler, Webserver)
âœ… No Redis, no Celery workers
âœ… Stable for learning & interviews
âœ… No laptop freezing ğŸ”¥

---

## ğŸ“¦ Folder structure (expected)

```
airflow-docker/
â”œâ”€â”€ dags/
â”œâ”€â”€ logs/
â”œâ”€â”€ plugins/
â”œâ”€â”€ config/
â”œâ”€â”€ docker-compose.yaml
â””â”€â”€ .env
```

---

## ğŸ”‘ .env (MUST BE EXACT)

```env
AIRFLOW_UID=50000
AIRFLOW_IMAGE_NAME=apache/airflow:2.8.4
```

---

## ğŸ³ OPTIMIZED docker-compose.yaml

```yaml
version: "3.8"

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME}
  env_file:
    - .env
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__LOAD_EXAMPLES: "true"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./config:/opt/airflow/config
  user: "${AIRFLOW_UID}:0"

services:

  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    depends_on:
      postgres:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.0"

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      postgres:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.0"

volumes:
  postgres-db-volume:
```

---

## â–¶ï¸ How to start (CLEAN WAY)

```powershell
docker compose down -v
docker system prune -f
docker compose up -d
```

Open UI ğŸ‘‰ **[http://localhost:8080](http://localhost:8080)**
Login: `airflow / airflow`

---

# ğŸ§  PRACTICE PLAN: BASIC â†’ INTERMEDIATE (INTERVIEWâ€‘READY)

## ğŸŸ¢ LEVEL 1 â€” Core Basics

âœ” What is DAG, Task, Operator
âœ” dag_id, start_date, schedule, catchup
âœ” BashOperator, PythonOperator

### Practice

* Create a DAG that prints todayâ€™s date
* Run manually vs scheduled
* Pause & unpause DAG

---

## ğŸŸ¡ LEVEL 2 â€” Dependencies & Control Flow

âœ” task1 >> task2
âœ” BranchPythonOperator
âœ” Trigger rules (`all_success`, `one_failed`)

### Practice

* Branch DAG: weekday vs weekend
* Fail a task and observe downstream behavior

---

## ğŸŸ¡ LEVEL 3 â€” Scheduling & Backfills

âœ” cron vs timedelta
âœ” catchup = true / false
âœ” backfill command

### Practice

* DAG that runs daily from Jan 1
* Enable catchup and observe runs

---

## ğŸŸ  LEVEL 4 â€” XComs & Variables

âœ” XCom push / pull
âœ” Airflow Variables
âœ” Connections (UI)

### Practice

* Pass value from Task A â†’ Task B
* Store env name in Variable

---

## ğŸŸ  LEVEL 5 â€” Sensors & External Triggers

âœ” FileSensor
âœ” TimeSensor
âœ” TriggerDagRunOperator

### Practice

* DAG waits for file â†’ then runs
* Trigger DAGâ€‘B from DAGâ€‘A

---

## ğŸ”µ LEVEL 6 â€” Failure Handling & Retry

âœ” retries, retry_delay
âœ” email_on_failure
âœ” SLA miss

### Practice

* Task fails first 2 times, passes 3rd

---

## ğŸ”µ LEVEL 7 â€” Realâ€‘World Mini Projects

### ğŸ”¥ Project 1: Fileâ€‘Driven Pipeline

* Sensor waits for CSV
* Python task validates data
* Bash task moves file

### ğŸ”¥ Project 2: Multiâ€‘DAG Orchestration

* Parent DAG triggers child DAG
* Child DAG returns status

### ğŸ”¥ Project 3: Parameterized DAG

* DAG takes runtime params
* Same DAG runs for dev/prod

---

##  GOLD LINES 

> â€œFor local development I use **Airflow with LocalExecutor via Docker Compose**, and switch to **CeleryExecutor in production**.â€

> â€œIâ€™ve handled scheduling, sensors, XComs, retries, and DAGâ€‘toâ€‘DAG orchestration.â€

---

## ğŸ Final Verdict

âœ… Optimized
âœ… Stable
âœ… Laptopâ€‘friendly
âœ… Interviewâ€‘ready

---

ğŸ‘‰ Next steps available:

* Readyâ€‘made **practice DAGs**
* **Interview Q&A** based on your setup
* **ADF vs Airflow comparison**

Say the word ğŸš€
