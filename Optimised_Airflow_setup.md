# ğŸš€ Apache Airflow on Windows using WSL2 (Optimized for 8â€¯GB RAM)

This guide explains **why** and **how** to run Apache Airflow on Windows using **WSL2**, how to **start it daily after restart**, and a **clear comparison of WSL2 vs Docker**.

This setup is:

* âœ… Stable
* âœ… Industryâ€‘correct
* âœ… Lightweight for 8â€¯GB RAM
* âœ… Closest to real production Linux environments

âš ï¸ **If anything goes wrong, always verify the *Important Notes* section below.**

---

# ğŸ’¥ Why NOT Docker? 

Docker is powerful â€” but **powerful does not mean appropriate for beginners**.

### ğŸ³ Why Docker Looks Attractive (But Isnâ€™t Ideal Initially)

Docker tutorials often promise:

* "One command setup"
* "Production-like environment"

In reality on Windows + 8 GB RAM:

* âŒ Containers compete heavily for memory
* âŒ Airflow + Docker + Windows = frequent crashes
* âŒ Debugging DAGs becomes harder
* âŒ File sync issues with volumes
* âŒ SQLite behaves poorly without careful volume tuning

You end up learning **Docker problems**, not **Airflow concepts**.

---

## ğŸ’¥ Why WSL2 Wins for Learning Airflow

WSL2 gives you:

* âœ… Real Linux kernel (same as prod VMs)
* âœ… Native filesystem performance
* âœ… No container abstraction confusion
* âœ… Simple Python debugging
* âœ… Perfect for SQLite + SequentialExecutor

You focus on:

* DAGs
* Operators
* Scheduling
* Dependencies

Not on:

* Ports
* Volumes
* Containers

---

## ğŸ’¥ Industry Reality (Important Truth)

* **Developers** often use Linux/WSL locally
* **Teams** use Docker only when collaboration is needed
* **Production** uses managed Airflow or Kubernetes

So the learning path should be:

```
WSL2 â†’ Airflow Concepts â†’ Docker â†’ Kubernetes / MWAA
```

Skipping WSL2 and jumping straight to Docker is like:

> Learning Kubernetes before learning Linux 

---

### ğŸ Verdict (From My Perspective):

> **Docker is NOT wrong. Itâ€™s just NOT step one.**

* ğŸŸ¢ WSL2 â†’ Best for beginners, interviews, solo learning
* ğŸŸ¡ Docker â†’ Best after you understand Airflow deeply

This guide intentionally uses **WSL2 first**, because **clarity beats complexity** every single time.

---

# ğŸ’¥ WSL2 vs Docker (CLEAR COMPARISON)

| Feature               | WSL2            | Docker       |
| --------------------- | --------------- | ------------ |
| Stability on Windows  | â­â­â­â­â­           | â­â­â­          |
| RAM usage             | Low             | High         |
| Setup complexity      | Simple          | Complex      |
| SQLite support        | Works perfectly | Needs tuning |
| Learning friendly     | âœ… Best          | âš ï¸ Heavy     |
| Production similarity | High            | Very High    |

**Verdict:**

* **WSL2** â†’ Best for learning, interviews, local dev
* **Docker** â†’ Best for team setups & prodâ€‘like orchestration

---


---

## ğŸ“Œ Why WSL2 (and NOT Native Windows)

Apache Airflow is **Linuxâ€‘native**.

Running Airflow directly on Windows often fails due to:

* Fileâ€‘locking issues (SQLite)
* NTFS permission conflicts
* Windows Defender / Antivirus interference
* Path and symlink problems

 **WSL2 provides a real Linux kernel**, so Airflow behaves exactly like it does on production servers.

---

# ğŸ’¥ WSL SetupğŸ› ï¸

## ğŸ“‚ Architecture Overview

```
Windows
 â””â”€â”€ WSL2 (Ubuntu â€“ Linux Kernel)
       â”œâ”€â”€ Python 3.10 (venv)
       â”œâ”€â”€ Apache Airflow
       â”œâ”€â”€ SQLite (metadata DB â€“ dev only)
       â”œâ”€â”€ Scheduler
       â””â”€â”€ Webserver (UI :8080)
```

---

## ğŸ§ª Prerequisites

* Windows 10 / 11
* Minimum **8â€¯GB RAM**
* Internet access
* WSL2 enabled

---

## âœ” STEP 1 â€” Install WSL2 + Ubuntu

Open **PowerShell as Administrator**:

```bash
wsl --install
```

Restart if prompted.

After restart:

1. Open **Ubuntu** from Start Menu
2. Create a Linux user (lowercase only)

Example:

```
username: karanwsl
password: ******
```

---

## âœ” STEP 2 â€” Install Python 3.10 (Supported Version)

Inside Ubuntu terminal:

```bash
sudo apt update
sudo apt install -y software-properties-common
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install -y python3.10 python3.10-venv python3-pip
```

Verify:

```bash
python3.10 --version
```

---

## âœ” STEP 3 â€” Create Airflow Project & Virtual Environment

```bash
cd ~
mkdir airflow
cd airflow
python3.10 -m venv venv
source venv/bin/activate
```

You should see:

```
(venv) user@DESKTOP:~/airflow$
```

---

## âœ” STEP 4 â€” Install Apache Airflow (Stable)

```bash
pip install apache-airflow==2.8.4 \
 --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.8.4/constraints-3.10.txt
```

---

## âœ” STEP 5 â€” Initialize Airflow

```bash
export AIRFLOW_HOME=~/airflow_home
airflow db init
```

Creates:

```
airflow.cfg
airflow.db
dags/
logs/
plugins/
```

---

## âœ” STEP 6 â€” Create Admin User

```bash
airflow users create \
  --username admin \
  --password admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@test.com
```

---

## âœ” STEP 7 â€” Start Airflow ServicesğŸš€

### Terminal 1 â€” Scheduler

```bash
cd ~/airflow
source venv/bin/activate
export AIRFLOW_HOME=~/airflow_home
airflow scheduler
```

### Terminal 2 â€” Webserver

```bash
cd ~/airflow
source venv/bin/activate
export AIRFLOW_HOME=~/airflow_home
airflow webserver -p 8080
```

---

## ğŸŒ STEP 8 â€” Open Airflow UI

Open browser on Windows:

```
http://localhost:8080
```

Login:

```
admin / admin
```

---

# ğŸ’¥ Daily Startup After PC RestartğŸ”

Every time you restart your PC:

### âœ” Open Ubuntu

```bash
wsl -d Ubuntu -u karanwsl
```

### âœ” Start Scheduler

```bash
cd ~/airflow
source venv/bin/activate
export AIRFLOW_HOME=~/airflow_home
airflow scheduler
```

### âœ” Start Webserver (New Terminal) 

```bash
cd ~/airflow
source venv/bin/activate
export AIRFLOW_HOME=~/airflow_home
airflow webserver -p 8080
```

### âœ” Open UI 

```
http://localhost:8080
```

---

## ğŸ’¥ Make AIRFLOW_HOME Permanent (Recommended)

```bash
nano ~/.bashrc
```

Add:

```bash
export AIRFLOW_HOME=$HOME/airflow_home
```

Apply:

```bash
source ~/.bashrc
```

---

# ğŸ’¥ The **Third Terminal** (Daily Developer Workflow)

One of the most common confusions for beginners is **which terminal is used for what**. This section clarifies the **third terminal**, its purpose, and why it is critical for learning and daily Airflow usage.

---

##  The Three-Terminal Mental Model (Industry Reality)

When working with Airflow locally, you should think in **three separate terminals**, each with a **single responsibility**.

###  âœ”  Terminal 1 â€” Scheduler (Brain)

Purpose:

* Continuously scans the `dags/` folder
* Parses DAG files
* Decides *when* tasks should run

Command:

```bash
airflow scheduler
```

Rule:

> âŒ Never edit files here
> âŒ Never stop it unless needed

---

### âœ”  Terminal 2 â€” Webserver (UI)

Purpose:

* Serves the Airflow UI (`localhost:8080`)
* Displays DAGs, task states, logs, graphs

Command:

```bash
airflow webserver -p 8080
```

Rule:

> âŒ Never write code here
> âŒ Restart only if UI glitches

---

### âœ”  Terminal 3 â€” **Developer / Working Terminal** (MOST IMPORTANT)

Purpose:

* Create and edit DAG files
* Clean DAGs
* Run validations
* Perform learning & experimentation

This is the terminal you actively **type in all day**.

Typical commands used **today**:

```bash
cd $AIRFLOW_HOME/dags
nano first_dag.py
nano dependency_dag.py
ls
python dependency_dag.py
touch dependency_dag.py
```

Use cases:

* Writing DAG logic
* Debugging missing DAGs
* Fixing syntax errors
* Cleaning example DAG clutter

Golden rule:

>  **Scheduler reads. Webserver shows. Third terminal builds.**

---

# ğŸ’¥ WHAT TO DO AFTER SETUP (VERY IMPORTANT)ğŸ§¹

## âœ” Disable Example DAGs (MANDATORY)

```bash
nano $AIRFLOW_HOME/airflow.cfg
```

Set:

```ini
load_examples = False
```

Restart scheduler + webserver.

---

## âœ” Delete Extra DAGs

```bash
rm -f $AIRFLOW_HOME/dags/*.py
```

Clean UI = faster learning.

---

## âœ” Create Your First DAG

```bash
cd $AIRFLOW_HOME/dags
nano first_dag.py
```

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id="first_dag",
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False
) as dag:

    hello = BashOperator(
        task_id="say_hello",
        bash_command="echo 'Hello Airflow ğŸš€'"
    )
```

Trigger it from UI and verify logs.

---

## âœ” Verify Correct Folder Usage

âœ… Always use:

```
/home/<user>/airflow_home
```

âŒ Never use:

```
/mnt/c/
```

---


## âš ï¸ Important Notes (READ THIS)

* SQLite + SequentialExecutor = **DEV ONLY**
* Never run Airflow from `/mnt/c`
* Always activate virtual environment
* Scheduler **and** Webserver must both be running

---

## ğŸ Final Verdict

WSL2 is the **most reliable, clean, and industryâ€‘correct** way to run Apache Airflow on a Windows laptop.

You now have a setup that:

âœ… Matches real Linux production
âœ… Avoids Windows filesystem bugs
âœ… Runs smoothly on 8â€¯GB RAM

---

## ğŸ¯ What To Do Next

* Create multiple DAGs
* Learn PythonOperator
* Understand Scheduler vs Executor
* Learn XCom, Sensors, Triggers
* Build a mini ETL pipeline
* 
---

## ğŸ’¥ CAN WE RUN AIRFLOW COMMANDS IN DETACHED MODE?

### âœ… **YES**, in multiple **Linux-native ways** inside WSL.

We have **4 correct options**.
Iâ€™ll rank them from **BEST â†’ BASIC**.

---


## ğŸ’¥ WHY DETACH MODE MATTERS FOR AIRFLOWâš ï¸

#### Airflow needs long-running processes:

* Scheduler
* Webserver

### You cannot sit and watch logs all day!!! 
So detach mode allows:
* âœ” Background execution
* âœ” Laptop sleep / resume
* âœ” Multiple terminals free

---

## ğŸ’¥ OPTION 1 â€” `tmux` (BEST & MOST USED)ğŸ¥‡

This is what **senior engineers actually use**.

### Why `tmux`?

âœ” Runs processes in background
âœ” Survives terminal close
âœ” Easy attach / detach
âœ” Lightweight (perfect for WSL)

---

## âœ”  Install tmux (one time)

```bash
sudo apt install -y tmux
```

---

###  **You CANNOT safely â€œattachâ€ an already-running Airflow scheduler/webserver to tmux**

If they were started in a **normal terminal**.

Why?

* Those processes are **bound to that terminalâ€™s TTY**
* tmux creates a **new virtual TTY**
* Linux does **not** support moving a live process between TTYs (by default)

âš ï¸ Tools like `reptyr` exist, but:

* require root
* unreliable
* NOT recommended
* NOT interview-expected

 **Professional practice is: stop â†’ restart in tmux**.

This is NOT a limitation of you â€” itâ€™s how Linux works.

---

## ğŸ’¥ CORRECT & PROFESSIONAL WAY (USED EVERYWHERE)

### âœ”  SCENARIO YOU ARE IN (If your terminals in running)

* Terminal 1 â†’ `airflow scheduler` running
* Terminal 2 â†’ `airflow webserver` running
* You want:

  * close terminals
  * keep Airflow running
  * see logs later

### âœ” The RIGHT solution:

 **Restart both inside tmux**

---

## ğŸ’¥ STEP-BY-STEP: MOVE AIRFLOW INTO DETACHED MODE (tmux)

## âœ”  STEP 1 â€” STOP CURRENT PROCESSES

In both terminals press:

```
CTRL + C
```

This stops:

* scheduler
* webserver

(Stopping is safe â€” no data loss.)

---

## âœ”  STEP 2 â€” START tmux SESSION

```bash
tmux new -s airflow
```

You are now **inside tmux**.

<img width="1851" height="430" alt="Screenshot 2025-12-18 163842" src="https://github.com/user-attachments/assets/754e117f-fd68-4857-b3d9-5e17f9bc5a97" />

---

## âœ”  STEP 3 â€” START SCHEDULER (INSIDE tmux)

```bash
cd ~/airflow
source venv/bin/activate
export AIRFLOW_HOME=~/airflow_home
airflow scheduler
```

<img width="1232" height="421" alt="Screenshot 2025-12-18 164041" src="https://github.com/user-attachments/assets/5c66d159-0826-4282-aa44-d1484dfee00a" />

---

## âœ”  STEP 4 â€” DETACH (KEEP IT RUNNING)

Press:

```
CTRL + B  â†’  D
```

Scheduler is now running **in background** âœ…

<img width="1218" height="54" alt="Screenshot 2025-12-18 164248" src="https://github.com/user-attachments/assets/35c4bcc4-b09e-4e49-b440-45cd02f6bf58" />

---

## âœ”  STEP 5 â€” ADD WEBSERVER IN SAME tmux SESSION

Reattach:

```bash
tmux attach -t airflow
```

Create new window:

```
CTRL + B  â†’  C
```

Now run:

```bash
cd ~/airflow
source venv/bin/activate
export AIRFLOW_HOME=~/airflow_home
airflow webserver -p 8080
```

<img width="1086" height="173" alt="Screenshot 2025-12-18 164437" src="https://github.com/user-attachments/assets/5489e385-c113-4eac-914d-f15ea36b0faa" />

Detach again:

```
CTRL + B â†’ D
```

<img width="1060" height="109" alt="Screenshot 2025-12-18 164452" src="https://github.com/user-attachments/assets/f6564c9c-4461-46a5-9830-1b31eb483927" />


---

###  âœ”  RESULT (THIS IS WHAT YOU WANT)ğŸ‰

âœ” You can close all terminals
âœ” Airflow keeps running
âœ” UI stays available at `localhost:8080`
âœ” Logs are visible when you reattach

<img width="1919" height="979" alt="Screenshot 2025-12-18 164531" src="https://github.com/user-attachments/assets/3f4b0438-051d-4d7b-8f40-22bfe72ab858" />

---

## ğŸ’¥ HOW TO SEE OUTPUTS LATER (VERY IMPORTANT)ğŸ”

## âœ”  Reattach anytime:

```bash
tmux attach -t airflow
```

Switch between:

* scheduler window
* webserver window

Using:

```
CTRL + B â†’ N   (next)
CTRL + B â†’ P   (previous)
```

---

## âœ”  TERMINATE tmux COMPLETELY (STOP EVERYTHING)âŒ

If you are done for now and want to stop Airflow:

```
tmux kill-session -t airflow
```

Now verify:
```
tmux ls
```

Expected:
```
no server running on /tmp/tmux-...
```
---

## âœ”  WHAT IF YOU REALLY DONâ€™T WANT TO RESTART?

Only alternative (not recommended):

```bash
nohup airflow scheduler > scheduler.log 2>&1 &
nohup airflow webserver -p 8080 > webserver.log 2>&1 &
```

Then view logs:

```bash
tail -f scheduler.log
tail -f webserver.log
```

âŒ No interaction
âŒ Harder debugging
âŒ Less professional

---

## ğŸ¯ VERY IMPORTANT

If asked:

> Can you move a running process to detached mode?

Answer:

> **â€œNo, a running process canâ€™t be retroactively attached to tmux. Best practice is to restart long-running services inside tmux or nohup.â€**


---


## ğŸ’¥ OPTION 2 â€” `nohup` (SIMPLE & EFFECTIVE)ğŸ¥ˆ

Good if you donâ€™t want tmux.

---

### Start scheduler

```bash
nohup airflow scheduler > scheduler.log 2>&1 &
```

### Start webserver

```bash
nohup airflow webserver -p 8080 > webserver.log 2>&1 &
```

Check running:

```bash
ps aux | grep airflow
```

Stop later:

```bash
pkill -f "airflow scheduler"
pkill -f "airflow webserver"
```

âš ï¸ Logs go into `.log` files.

---

## ğŸ’¥ OPTION 3 â€” `&` (VERY BASIC)ğŸ¥‰

```bash
airflow scheduler &
airflow webserver -p 8080 &
```

âš ï¸ Process **dies if terminal closes**
âŒ Not recommended

---

## ğŸ’¥ OPTION 4 â€” systemd (NOT RECOMMENDED)ğŸ…

Possible in WSL, but:

* complex
* overkill
* not needed for learning

ğŸš« Skip this.

---

# ğŸ† RECOMMENDED 

### **tmux** âœ…

Because:

* Youâ€™re learning
* You restart laptop
* You want control
* You want professional practice

---

# ğŸ¯ QUICK DECISION GUIDE

| Tool   | Use case             |
| ------ | -------------------- |
| tmux   | â­ BEST (recommended) |
| nohup  | Simple background    |
| &      | Temporary            |
| Docker | Production           |

---

















